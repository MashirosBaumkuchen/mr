{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "data['datetime'] = data['time'].apply(lambda x: str(x).split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate 1\n",
    "posi_f = [\n",
    "    'voice_connection', 'wifi_connection',\n",
    "    'voice_convert_1', 'convert_rate',\n",
    "    'rrc_connection', 'erab_connection',\n",
    "    'esrvcc_convert'\n",
    "]\n",
    "\n",
    "# rate 0\n",
    "navg_f = [\n",
    "    'voice_disconnection', 'wifi_disconnection',\n",
    "    'wifi_disconnection_1',\n",
    "    'erab_trash', 'prb_pull', 'prb_push'\n",
    "]\n",
    "\n",
    "# count \n",
    "count_f = [\n",
    "    'voice_pull_delay','voice_count', 'data_count', \n",
    "    'rrc_max', 'csgb_rrc', 'rrc_2g', 'rrc_3g', 'rrc_num',\n",
    "    'voice_push_miss', 'voice_pull_miss'\n",
    "]\n",
    "\n",
    "# drop\n",
    "drop_f = [\n",
    "    'video_connection', 'video_disconnection', 'voice_convert_2', 'voice_convert_2', 'pdcch_cce'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_f = posi_f + navg_f + count_f\n",
    "\n",
    "cat_f = [\n",
    "    'ENODEB_ID', 'CID', 'hour'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_(dataframe, feature, fillna='0.0', astype=np.float32, normalize=True):\n",
    "    dataframe[feature] = dataframe[feature].fillna(fillna)\n",
    "    dataframe[feature] = dataframe[feature].astype(np.float32)\n",
    "    if normalize:\n",
    "        dataframe.loc[dataframe[feature]>1, feature] = 1\n",
    "        dataframe.loc[dataframe[feature]<0, feature] = 0\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalized_feature(dataframe, feature):\n",
    "    mms = MinMaxScaler()\n",
    "    return mms.fit_transform(dataframe[feature].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_feature(dataframe, datetime):\n",
    "    tmp = dataframe[dataframe.loc[:,('datetime')]==datetime]\n",
    "    tmp = tmp.drop(['datetime'], axis=1)\n",
    "    \n",
    "    for feature in posi_f:\n",
    "        tmp = format_(tmp, feature, fillna=tmp[feature].mean())\n",
    "    for feature in navg_f:\n",
    "        tmp = format_(tmp, feature, fillna=tmp[feature].mean())\n",
    "    for feature in count_f:\n",
    "        tmp = format_(tmp, feature, fillna=tmp[feature].mean(), normalize=False)\n",
    "        tmp[feature] = normalized_feature(tmp, feature)\n",
    "    tmp = tmp.drop(drop_f, axis=1)\n",
    "\n",
    "    tmp.mr_low = tmp.mr_low.astype(np.float32)\n",
    "    tmp.mr_high = tmp.mr_high.astype(np.float32)\n",
    "    \n",
    "    # tmp['MCC'] = tmp['cgi'].apply(lambda x: str(x).split('-')[0])\n",
    "    # tmp['MNC'] = tmp['cgi'].apply(lambda x: str(x).split('-')[1])\n",
    "    tmp['ENODEB_ID'] = tmp['cgi'].apply(lambda x: str(x).split('-')[2])\n",
    "    tmp['CID'] = tmp['cgi'].apply(lambda x: str(x).split('-')[3])\n",
    "    tmp['ENODEB_ID'] = tmp['ENODEB_ID'].astype(np.int32)\n",
    "    tmp['CID'] = tmp['CID'].astype(np.int32)\n",
    "\n",
    "    # tmp['month'] = pd.to_datetime(tmp['time']).dt.month\n",
    "    # tmp['day'] = pd.to_datetime(tmp['time']).dt.day\n",
    "    tmp['hour'] = pd.to_datetime(tmp['time']).dt.hour\n",
    "\n",
    "    tmp = tmp.drop(['city', 'region', 'cgi', 'time'], axis=1)\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_valid):\n",
    "    train = creat_feature(data, '2018-05-04')\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-03')])\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-02')])\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-01')])\n",
    "    train = pd.concat([train, creat_feature(data, '2018-04-30')])\n",
    "    test = creat_feature(data, '2018-05-05')\n",
    "else:\n",
    "    train = creat_feature(data, '2018-05-05')\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-04')])\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-03')])\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-02')])\n",
    "    train = pd.concat([train, creat_feature(data, '2018-05-01')])\n",
    "    test = creat_feature(data, '2018-05-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6028403, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1214923, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['voice_connection', 'wifi_connection', 'voice_disconnection',\n",
       "       'wifi_disconnection', 'esrvcc_convert', 'voice_convert_1',\n",
       "       'convert_rate', 'voice_push_miss', 'voice_pull_miss',\n",
       "       'voice_pull_delay', 'voice_count', 'data_count', 'rrc_connection',\n",
       "       'erab_connection', 'erab_trash', 'wifi_disconnection_1', 'prb_push',\n",
       "       'prb_pull', 'rrc_max', 'csgb_rrc', 'rrc_2g', 'rrc_3g', 'rrc_num',\n",
       "       'mr_low', 'mr_high', 'ENODEB_ID', 'CID', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1 = train['mr_low']\n",
    "train_y2 = train['mr_high']\n",
    "train_X = train.drop(['mr_low', 'mr_high'], axis=1)\n",
    "\n",
    "test_y1 = test['mr_low']\n",
    "test_y2 = test['mr_high']\n",
    "test_X = test.drop(['mr_low', 'mr_high'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "#     'application': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'zero_as_missing': True,\n",
    "#     'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'metric':{'mse'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_predict(X,y,X_pred):\n",
    "    predictors = [i for i in X.columns]\n",
    "    stacking_num = 2\n",
    "    bagging_num = 3\n",
    "    bagging_test_size = 0.33\n",
    "    num_boost_round = 500\n",
    "    early_stopping_rounds = 100\n",
    "                \n",
    "    stacking_model=[]\n",
    "    bagging_model=[]\n",
    "\n",
    "    l2_error = []\n",
    "#     X = X.values\n",
    "#     y = y.values\n",
    "    layer_train = np.zeros((X.shape[0],2))\n",
    "    \n",
    "    leng = X.shape[0]\n",
    "        \n",
    "    for i in range(2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "        \n",
    "        lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=cat_f)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, categorical_feature=cat_f)\n",
    "        \n",
    "        gbm=lgb.train(param,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=1000,\n",
    "                      valid_sets=lgb_eval,\n",
    "                      verbose_eval=50,\n",
    "                      early_stopping_rounds=100)\n",
    "        stacking_model.append(gbm)\n",
    "    X = np.hstack((X,layer_train[:,1].reshape((-1,1))))\n",
    "    \n",
    "    predictors.append('lgb_result')\n",
    "    \n",
    "    for bn in range(bagging_num):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=bagging_test_size, random_state=bn)\n",
    "        \n",
    "        lgb_train = lgb.Dataset(X_train,y_train, categorical_feature=cat_f)\n",
    "        lgb_eval = lgb.Dataset(X_test,y_test, categorical_feature=cat_f)\n",
    "        \n",
    "        gbm = lgb.train(param,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=1000,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=50,\n",
    "                        early_stopping_rounds=100)\n",
    "        bagging_model.append(gbm)\n",
    "        l2_error.append(mean_squared_error(gbm.predict(X_test,num_iteration=gbm.best_iteration),y_test))\n",
    "        \n",
    "        feat_imp = pd.Series(gbm.feature_importance(), predictors).sort_values(ascending=False)\n",
    "        \n",
    "    test_pred = np.zeros((X_pred.shape[0],stacking_num))\n",
    "    for sn,gbm in enumerate(stacking_model):\n",
    "        pred = gbm.predict(X_pred,num_iteration=gbm.best_iteration)\n",
    "        test_pred[:,sn] = pred\n",
    "        \n",
    "        X_pred = np.hstackk((X_pred,test_pred.mean(axis=1).reshape((-1,1))))\n",
    "        \n",
    "    for bn,gbm in enumerate(bagging_model):\n",
    "        pred = gbm.predict(X_pred,num_iteration=gbm.best_iteration)\n",
    "        if bn==0:\n",
    "            pred_out = pred\n",
    "        else:\n",
    "            pred_out += pred\n",
    "    return pred_out/bagging_num, feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihave4cat/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/ihave4cat/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 0.00393412\n",
      "[100]\tvalid_0's l2: 0.00349464\n",
      "[150]\tvalid_0's l2: 0.00328078\n",
      "[200]\tvalid_0's l2: 0.00314694\n",
      "[250]\tvalid_0's l2: 0.00305589\n",
      "[300]\tvalid_0's l2: 0.00297188\n",
      "[350]\tvalid_0's l2: 0.00291256\n",
      "[400]\tvalid_0's l2: 0.00285894\n",
      "[450]\tvalid_0's l2: 0.00282231\n",
      "[500]\tvalid_0's l2: 0.00278722\n",
      "[550]\tvalid_0's l2: 0.00275929\n",
      "[600]\tvalid_0's l2: 0.00273091\n",
      "[650]\tvalid_0's l2: 0.00270544\n",
      "[700]\tvalid_0's l2: 0.0026836\n",
      "[750]\tvalid_0's l2: 0.00266404\n",
      "[800]\tvalid_0's l2: 0.00264816\n",
      "[850]\tvalid_0's l2: 0.00263191\n",
      "[900]\tvalid_0's l2: 0.00261184\n",
      "[950]\tvalid_0's l2: 0.00260179\n",
      "[1000]\tvalid_0's l2: 0.00259201\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 0.00259201\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 0.00385582\n",
      "[100]\tvalid_0's l2: 0.00340345\n",
      "[150]\tvalid_0's l2: 0.0031856\n",
      "[200]\tvalid_0's l2: 0.0030441\n",
      "[250]\tvalid_0's l2: 0.00295668\n",
      "[300]\tvalid_0's l2: 0.00288745\n",
      "[350]\tvalid_0's l2: 0.00282542\n",
      "[400]\tvalid_0's l2: 0.00277803\n",
      "[450]\tvalid_0's l2: 0.00273776\n",
      "[500]\tvalid_0's l2: 0.0027035\n",
      "[550]\tvalid_0's l2: 0.0026741\n",
      "[600]\tvalid_0's l2: 0.0026464\n",
      "[650]\tvalid_0's l2: 0.00262524\n",
      "[700]\tvalid_0's l2: 0.00260458\n",
      "[750]\tvalid_0's l2: 0.00258915\n",
      "[800]\tvalid_0's l2: 0.0025742\n",
      "[850]\tvalid_0's l2: 0.00255979\n",
      "[900]\tvalid_0's l2: 0.00254836\n",
      "[950]\tvalid_0's l2: 0.0025353\n",
      "[1000]\tvalid_0's l2: 0.00252349\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 0.00252349\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 0.00392836\n",
      "[100]\tvalid_0's l2: 0.00348212\n",
      "[150]\tvalid_0's l2: 0.00327011\n",
      "[200]\tvalid_0's l2: 0.00312642\n",
      "[250]\tvalid_0's l2: 0.00303789\n",
      "[300]\tvalid_0's l2: 0.0029623\n",
      "[350]\tvalid_0's l2: 0.00290684\n",
      "[400]\tvalid_0's l2: 0.00285609\n",
      "[450]\tvalid_0's l2: 0.00281551\n",
      "[500]\tvalid_0's l2: 0.00278216\n",
      "[550]\tvalid_0's l2: 0.0027503\n",
      "[600]\tvalid_0's l2: 0.00272599\n",
      "[650]\tvalid_0's l2: 0.00270451\n",
      "[700]\tvalid_0's l2: 0.00268466\n",
      "[750]\tvalid_0's l2: 0.0026671\n",
      "[800]\tvalid_0's l2: 0.00264122\n",
      "[850]\tvalid_0's l2: 0.00262897\n",
      "[900]\tvalid_0's l2: 0.00261409\n",
      "[950]\tvalid_0's l2: 0.00260171\n",
      "[1000]\tvalid_0's l2: 0.00258905\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 0.00258905\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's l2: 0.00390838\n",
      "[100]\tvalid_0's l2: 0.00346435\n",
      "[150]\tvalid_0's l2: 0.00324362\n",
      "[200]\tvalid_0's l2: 0.00308933\n",
      "[250]\tvalid_0's l2: 0.00299482\n",
      "[300]\tvalid_0's l2: 0.00293073\n",
      "[350]\tvalid_0's l2: 0.00286862\n",
      "[400]\tvalid_0's l2: 0.00282027\n",
      "[450]\tvalid_0's l2: 0.00278141\n",
      "[500]\tvalid_0's l2: 0.00274895\n",
      "[550]\tvalid_0's l2: 0.00271575\n",
      "[600]\tvalid_0's l2: 0.00269029\n",
      "[650]\tvalid_0's l2: 0.00266668\n",
      "[700]\tvalid_0's l2: 0.00264716\n",
      "[750]\tvalid_0's l2: 0.00262864\n",
      "[800]\tvalid_0's l2: 0.00261357\n",
      "[850]\tvalid_0's l2: 0.00259954\n",
      "[900]\tvalid_0's l2: 0.00258206\n",
      "[950]\tvalid_0's l2: 0.0025709\n"
     ]
    }
   ],
   "source": [
    "pre, imp = fit_predict(train_X, train_y1, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
